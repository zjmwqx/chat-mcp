"""
Step3æµ‹è¯•ï¼šéªŒè¯AI Provideré›†æˆå’ŒMCPå·¥å…·ä¼ é€’åŠŸèƒ½
æµ‹è¯•å°†MCPå·¥å…·ä¼ é€’ç»™AI Providerçš„å®Œæ•´æµç¨‹
"""

import asyncio
import logging
from typing import List

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from chat_mcp import (
    ChatMCPClient,
    MCPServer,
    MCPTool,
    ChatRequest,
    ChatResponse,
    ChatMessage,
    build_system_prompt,
    parse_tool_use,
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class MockMCPService:
    """æ¨¡æ‹ŸMCPæœåŠ¡ï¼Œç”¨äºæµ‹è¯•"""

    def __init__(self):
        self.mock_tools = [
            MCPTool(
                id="f123456789",
                name="search_arxiv",
                description="Search for academic papers on arXiv",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Search query for papers",
                        },
                        "max_results": {
                            "type": "integer",
                            "description": "Maximum number of results to return",
                            "default": 5,
                        },
                    },
                    "required": ["query"],
                },
                server_id="arxiv-server",
                server_name="ArXiv MCP Server",
            ),
            MCPTool(
                id="f987654321",
                name="get_weather",
                description="Get current weather information for a location",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "City name or coordinates",
                        },
                        "units": {
                            "type": "string",
                            "description": "Temperature units (celsius/fahrenheit)",
                            "default": "celsius",
                        },
                    },
                    "required": ["location"],
                },
                server_id="weather-server",
                server_name="Weather MCP Server",
            ),
        ]

    async def list_tools(self, server: MCPServer) -> List[MCPTool]:
        """æ¨¡æ‹Ÿåˆ—å‡ºå·¥å…·"""
        logger.info(f"[Mock] Listing tools for server: {server.name}")
        if "arxiv" in server.name.lower():
            return [self.mock_tools[0]]
        elif "weather" in server.name.lower():
            return [self.mock_tools[1]]
        else:
            return self.mock_tools


class MockAIProvider:
    """æ¨¡æ‹ŸAI Providerï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿæç¤ºè¯æ„å»º"""

    def __init__(self):
        pass

    async def completions(
        self,
        messages: List[ChatMessage],
        model: str = "gpt-3.5-turbo",
        mcp_tools: List[MCPTool] = None,
        temperature: float = 0.7,
        stream: bool = False,
        on_chunk=None,
    ) -> ChatResponse:
        """æ¨¡æ‹ŸAIå®Œæˆè°ƒç”¨"""
        logger.info(
            f"[MockAI] Received {len(messages)} messages with {len(mcp_tools) if mcp_tools else 0} tools"
        )

        # æ£€æŸ¥ç³»ç»Ÿæ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·ä¿¡æ¯
        system_message = None
        for msg in messages:
            if msg.role == "system":
                system_message = msg.content
                break

        # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨å“åº”
        if mcp_tools and len(mcp_tools) > 0:
            # å¦‚æœæœ‰å·¥å…·ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªå·¥å…·è°ƒç”¨
            mock_response = f"""æˆ‘çœ‹åˆ°æœ‰ {len(mcp_tools)} ä¸ªå¯ç”¨å·¥å…·ã€‚è®©æˆ‘ä½¿ç”¨å…¶ä¸­ä¸€ä¸ªå·¥å…·æ¥å¸®åŠ©æ‚¨ï¼š

<tool_use>
<tool_name>{mcp_tools[0].name}</tool_name>
<parameters>
{{
  "query": "machine learning",
  "max_results": 3
}}
</parameters>
</tool_use>

æˆ‘å·²ç»è°ƒç”¨äº† {mcp_tools[0].name} å·¥å…·æ¥æœç´¢ç›¸å…³ä¿¡æ¯ã€‚"""
        else:
            mock_response = "æˆ‘æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œç›®å‰æ²¡æœ‰å¯ç”¨çš„å·¥å…·ã€‚"

        # è§£æå·¥å…·è°ƒç”¨
        tool_calls = parse_tool_use(mock_response)

        response_message = ChatMessage(
            role="assistant",
            content=mock_response,
            metadata={
                "system_message_length": len(system_message) if system_message else 0,
                "has_tool_info": "Available tools:" in (system_message or ""),
                "mcp_tools_count": len(mcp_tools) if mcp_tools else 0,
                "parsed_tool_calls": tool_calls,
            },
        )

        return ChatResponse(
            message=response_message,
            usage={"mock": True, "tokens": 100},
            metrics={"step": 3, "status": "mock_completion"},
        )


async def test_system_prompt_building():
    """æµ‹è¯•ç³»ç»Ÿæç¤ºè¯æ„å»ºåŠŸèƒ½"""
    print("\n=== æµ‹è¯•1: ç³»ç»Ÿæç¤ºè¯æ„å»ºåŠŸèƒ½ ===")

    # åˆ›å»ºæµ‹è¯•å·¥å…·
    tools = [
        MCPTool(
            id="test1",
            name="search_papers",
            description="Search academic papers",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "limit": {"type": "integer", "description": "Result limit"},
                },
                "required": ["query"],
            },
            server_id="test-server",
            server_name="Test Server",
        )
    ]

    # æµ‹è¯•æ„å»ºç³»ç»Ÿæç¤ºè¯
    original_prompt = "You are a helpful assistant."
    enhanced_prompt = build_system_prompt(original_prompt, tools)

    original_len = len(original_prompt)
    enhanced_len = len(enhanced_prompt)

    print(f"[éªŒè¯] ç³»ç»Ÿæç¤ºè¯ä» {original_len} å­—ç¬¦å¢å¼ºåˆ° " f"{enhanced_len} å­—ç¬¦")
    print(f"åŒ…å«å·¥å…·ä¿¡æ¯: {'Available tools:' in enhanced_prompt}")
    print(f"åŒ…å«å·¥å…·åç§°: {'search_papers' in enhanced_prompt}")
    print(f"åŒ…å«XMLç¤ºä¾‹: {'<tool_use>' in enhanced_prompt}")

    # æµ‹è¯•æ²¡æœ‰å·¥å…·çš„æƒ…å†µ
    no_tools_prompt = build_system_prompt(original_prompt, [])
    assert no_tools_prompt == original_prompt, "æ²¡æœ‰å·¥å…·æ—¶åº”è¯¥è¿”å›åŸå§‹æç¤ºè¯"

    print("âœ… ç³»ç»Ÿæç¤ºè¯æ„å»ºæµ‹è¯•é€šè¿‡")


async def test_tool_use_parsing():
    """æµ‹è¯•å·¥å…·è°ƒç”¨è§£æåŠŸèƒ½"""
    print("\n=== æµ‹è¯•2: å·¥å…·è°ƒç”¨è§£æåŠŸèƒ½ ===")

    # æ¨¡æ‹ŸLLMå“åº”åŒ…å«å·¥å…·è°ƒç”¨
    llm_response = """
    æˆ‘æ¥å¸®ä½ æœç´¢å­¦æœ¯è®ºæ–‡ã€‚

    <tool_use>
    <tool_name>search_arxiv</tool_name>
    <parameters>
    {"query": "machine learning", "max_results": 5}
    </parameters>
    </tool_use>

    è®©æˆ‘ä¹ŸæŸ¥çœ‹ä¸€ä¸‹å¤©æ°”ã€‚

    <tool_use>
    <tool_name>get_weather</tool_name>
    <parameters>
    {"location": "Beijing", "units": "celsius"}
    </parameters>
    </tool_use>
    """

    # ä½¿ç”¨è§£æå‡½æ•°
    parsed_tools = parse_tool_use(llm_response)

    print(f"è§£æåˆ°çš„å·¥å…·è°ƒç”¨æ•°é‡: {len(parsed_tools)}")

    expected_calls = [
        {
            "name": "search_arxiv",
            "parameters": {"query": "machine learning", "max_results": 5},
        },
        {
            "name": "get_weather",
            "parameters": {"location": "Beijing", "units": "celsius"},
        },
    ]

    for i, expected in enumerate(expected_calls):
        assert i < len(parsed_tools), f"ç¼ºå°‘ç¬¬{i+1}ä¸ªå·¥å…·è°ƒç”¨"
        actual = parsed_tools[i]
        # ä¿®æ”¹è®¿é—®æ–¹å¼ï¼šä½¿ç”¨å¯¹è±¡å±æ€§è€Œä¸æ˜¯å­—å…¸é”®
        assert (
            actual.tool.name == expected["name"]
        ), f"å·¥å…·åç§°ä¸åŒ¹é…: {actual.tool.name} != {expected['name']}"
        assert (
            actual.tool.arguments == expected["parameters"]
        ), f"å‚æ•°ä¸åŒ¹é…: {actual.tool.arguments} != {expected['parameters']}"
        print(f"âœ… å·¥å…·è°ƒç”¨ {i+1}: {actual.tool.name} - å‚æ•°è§£ææ­£ç¡®")

    print("âœ… å·¥å…·è°ƒç”¨è§£ææµ‹è¯•é€šè¿‡")


async def test_ai_provider_integration():
    """æµ‹è¯•AI Provideré›†æˆ"""
    print("\n=== æµ‹è¯•3: AI Provideré›†æˆ ===")

    # åˆ›å»ºæ¨¡æ‹Ÿçš„AI Provider
    mock_ai = MockAIProvider()

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    messages = [
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="å¸®æˆ‘æœç´¢ä¸€äº›æœºå™¨å­¦ä¹ çš„è®ºæ–‡"),
    ]

    tools = [
        MCPTool(
            id="test1",
            name="search_arxiv",
            description="Search academic papers on arXiv",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum results",
                    },
                },
                "required": ["query"],
            },
            server_id="arxiv-server",
            server_name="ArXiv Server",
        )
    ]

    # æµ‹è¯•AIè°ƒç”¨
    response = await mock_ai.completions(
        messages=messages, model="gpt-3.5-turbo", mcp_tools=tools, temperature=0.7
    )

    print(f"å“åº”ç”ŸæˆæˆåŠŸ: {response.message.role == 'assistant'}")
    print(f"åŒ…å«å·¥å…·è°ƒç”¨: {'<tool_use>' in response.message.content}")
    print(
        f"å…ƒæ•°æ®åŒ…å«å·¥å…·ä¿¡æ¯: {response.message.metadata.get('mcp_tools_count', 0) > 0}"
    )
    print(
        f"æ£€æµ‹åˆ°çš„å·¥å…·è°ƒç”¨: {len(response.message.metadata.get('parsed_tool_calls', []))}"
    )

    print("âœ… AI Provideré›†æˆæµ‹è¯•é€šè¿‡")


async def test_chat_client_full_flow():
    """æµ‹è¯•å®Œæ•´çš„èŠå¤©å®¢æˆ·ç«¯æµç¨‹"""
    print("\n=== æµ‹è¯•4: å®Œæ•´èŠå¤©å®¢æˆ·ç«¯æµç¨‹ ===")

    # åˆ›å»ºèŠå¤©å®¢æˆ·ç«¯ï¼Œä½†ä½¿ç”¨æ¨¡æ‹ŸæœåŠ¡
    client = ChatMCPClient()

    # æ›¿æ¢ä¸ºæ¨¡æ‹ŸæœåŠ¡
    client.tool_collector.mcp_service = MockMCPService()
    client.ai_provider = MockAIProvider()

    # å‡†å¤‡æµ‹è¯•è¯·æ±‚
    servers = [
        MCPServer(
            id="arxiv-1",
            name="ArXiv MCP Server",
            command="python",
            args=["-m", "mcp_arxiv"],
            disabled_tools=[],
        ),
        MCPServer(
            id="weather-1",
            name="Weather MCP Server",
            command="python",
            args=["-m", "mcp_weather"],
            disabled_tools=["get_forecast"],  # ç¦ç”¨éƒ¨åˆ†å·¥å…·
        ),
    ]

    request = ChatRequest(
        messages=[
            ChatMessage(
                role="user", content="å¸®æˆ‘æœç´¢æœºå™¨å­¦ä¹ ç›¸å…³çš„è®ºæ–‡ï¼Œå¹¶å‘Šè¯‰æˆ‘åŒ—äº¬çš„å¤©æ°”"
            )
        ],
        model="gpt-3.5-turbo",
        enabled_mcps=servers,
        temperature=0.7,
    )

    # æ‰§è¡ŒèŠå¤©
    try:
        response = await client.chat(request)

        print(f"èŠå¤©æ‰§è¡ŒæˆåŠŸ: {response.message.role == 'assistant'}")
        print(f"æ­¥éª¤æ ‡è¯†: {response.message.metadata.get('step')}")
        print(f"å¯ç”¨æœåŠ¡å™¨æ•°é‡: {response.message.metadata.get('enabled_servers')}")
        print(
            f"æ”¶é›†çš„å·¥å…·æ•°é‡: {response.message.metadata.get('collected_tools_count')}"
        )
        print(f"é˜¶æ®µ: {response.message.metadata.get('stage')}")

        # éªŒè¯å…ƒæ•°æ®
        assert response.message.metadata.get("step") == 3, "æ­¥éª¤åº”è¯¥æ˜¯3"
        assert (
            response.message.metadata.get("enabled_servers") == 2
        ), "åº”è¯¥æœ‰2ä¸ªå¯ç”¨çš„æœåŠ¡å™¨"
        assert (
            response.message.metadata.get("collected_tools_count") >= 1
        ), "åº”è¯¥æ”¶é›†åˆ°è‡³å°‘1ä¸ªå·¥å…·"

        print("âœ… å®Œæ•´èŠå¤©æµç¨‹æµ‹è¯•é€šè¿‡")

    except Exception as e:
        print(f"âŒ èŠå¤©æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        raise


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n=== æµ‹è¯•5: é”™è¯¯å¤„ç† ===")

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = ChatMCPClient()
    client.tool_collector.mcp_service = MockMCPService()

    # åˆ›å»ºä¸€ä¸ªä¼šå¤±è´¥çš„AI Provider
    class FailingAIProvider:
        async def completions(self, **kwargs):
            raise Exception("æ¨¡æ‹ŸAIè°ƒç”¨å¤±è´¥")

    client.ai_provider = FailingAIProvider()

    # å‡†å¤‡è¯·æ±‚
    request = ChatRequest(
        messages=[ChatMessage(role="user", content="æµ‹è¯•é”™è¯¯å¤„ç†")],
        model="gpt-3.5-turbo",
        enabled_mcps=[
            MCPServer(id="test-server", name="Test Server", command="test", args=[])
        ],
    )

    # æ‰§è¡ŒèŠå¤©ï¼Œåº”è¯¥å¤„ç†é”™è¯¯
    response = await client.chat(request)

    print(f"é”™è¯¯å¤„ç†æˆåŠŸ: {'å¤±è´¥' in response.message.content}")
    print(f"é”™è¯¯ä¿¡æ¯ä¿å­˜: {response.message.metadata.get('error') is not None}")
    print(f"é”™è¯¯çŠ¶æ€: {response.metrics.get('status')}")

    assert "å¤±è´¥" in response.message.content, "åº”è¯¥åŒ…å«å¤±è´¥ä¿¡æ¯"
    assert response.message.metadata.get("error") is not None, "åº”è¯¥ä¿å­˜é”™è¯¯ä¿¡æ¯"

    print("âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹Step3æµ‹è¯• - AI Provideré›†æˆå’ŒMCPå·¥å…·ä¼ é€’")
    print("=" * 60)

    tests = [
        test_system_prompt_building,
        test_tool_use_parsing,
        test_ai_provider_integration,
        test_chat_client_full_flow,
        test_error_handling,
    ]

    passed = 0
    failed = 0

    for test in tests:
        try:
            await test()
            passed += 1
        except Exception as e:
            failed += 1
            print(f"âŒ æµ‹è¯•å¤±è´¥: {test.__name__}")
            print(f"é”™è¯¯: {e}")
            import traceback

            traceback.print_exc()

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")

    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰Step3æµ‹è¯•é€šè¿‡ï¼AI Provideré›†æˆå’ŒMCPå·¥å…·ä¼ é€’åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("\nâœ¨ Step3å®Œæˆæƒ…å†µ:")
        print("- âœ… ç³»ç»Ÿæç¤ºè¯æ„å»º - å°†MCPå·¥å…·ä¿¡æ¯åµŒå…¥åˆ°ç³»ç»Ÿæç¤ºè¯")
        print("- âœ… å·¥å…·è°ƒç”¨è§£æ - ä»LLMå“åº”ä¸­è§£æXMLæ ¼å¼çš„å·¥å…·è°ƒç”¨")
        print("- âœ… AI Provideré›†æˆ - å°†MCPå·¥å…·ä¼ é€’ç»™LiteLLM")
        print("- âœ… å®Œæ•´èŠå¤©æµç¨‹ - ç«¯åˆ°ç«¯çš„å·¥å…·æ”¶é›†â†’AIè°ƒç”¨â†’å“åº”å¤„ç†")
        print("- âœ… é”™è¯¯å¤„ç† - ä¼˜é›…å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ")

        print("\nğŸ”§ æŠ€æœ¯è¦ç‚¹:")
        print("- ä½¿ç”¨XMLæ ¼å¼çš„å·¥å…·è°ƒç”¨ï¼Œè€ŒéOpenAIçš„å‡½æ•°è°ƒç”¨")
        print("- æ”¯æŒå¤šå·¥å…·å¹¶è¡Œä¼ é€’ç»™LLM")
        print("- ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿åŒ–ï¼ŒåŒ…å«å·¥å…·è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹")
        print("- æ­£åˆ™è¡¨è¾¾å¼è§£æå·¥å…·è°ƒç”¨ï¼Œæ”¯æŒå¤æ‚å‚æ•°")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å’Œä¿®å¤")
        return 1

    return 0


if __name__ == "__main__":
    asyncio.run(main())
