"""
AI Provideræ¨¡å—ï¼šå®ç°LLMè°ƒç”¨å’ŒMCPå·¥å…·é›†æˆ
"""

import asyncio
import json
import re
import logging
import os
from typing import List, Dict, Any, Optional, Callable
from dotenv import load_dotenv

import litellm

from .mcp_types import (
    MCPTool,
    ChatMessage,
    ChatResponse,
    MCPToolCall,
    MCPCallToolResponse,
    ToolParseResult,
    MCPServer,
    MCPToolResponse,
)
from .mcp_service import get_mcp_service

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = logging.getLogger(__name__)

# ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
SYSTEM_PROMPT_TEMPLATE = """{{ USER_SYSTEM_PROMPT }}

{{ TOOL_USE_EXAMPLES }}

{{ AVAILABLE_TOOLS }}"""

# å·¥å…·ä½¿ç”¨ç¤ºä¾‹
TOOL_USE_EXAMPLES = """
You have access to tools that you can use to help answer questions. 
When using a tool, format your request using XML tags:

<tool_use>
<tool_name>tool_name_here</tool_name>
<parameters>
{
  "required_parameter": "value",
  "optional_parameter": "value_if_needed"
}
</parameters>
</tool_use>

IMPORTANT RULES:
1. Only call tools when you need additional information
2. After receiving tool results (success or error), provide your final answer directly
3. Do NOT retry failed tool calls or call the same tool multiple times
4. For optional parameters: ONLY include them if you have a specific value - 
   do NOT pass empty strings "", null, or placeholder values
5. When tool parameters are not mentioned or not needed, 
   simply omit them from the parameters object
"""

# å…¨å±€æœåŠ¡å™¨é…ç½®å­˜å‚¨
_server_configs: Dict[str, MCPServer] = {}


def register_server_config(server: MCPServer) -> None:
    """æ³¨å†ŒæœåŠ¡å™¨é…ç½®"""
    _server_configs[server.id] = server


def get_server_config(server_id: str) -> Optional[MCPServer]:
    """è·å–æœåŠ¡å™¨é…ç½®"""
    return _server_configs.get(server_id)


def build_available_tools_prompt(tools: List[MCPTool]) -> str:
    """
    æ„å»ºå¯ç”¨å·¥å…·çš„æç¤ºè¯

    Args:
        tools: MCPå·¥å…·åˆ—è¡¨

    Returns:
        æ ¼å¼åŒ–çš„å·¥å…·æè¿°å­—ç¬¦ä¸²
    """
    if not tools:
        return ""

    tools_desc = "Available tools:\n\n"
    for tool in tools:
        tools_desc += f"- **{tool.name}**: {tool.description}\n"

        # æ·»åŠ å‚æ•°æè¿°
        if tool.inputSchema and "properties" in tool.inputSchema:
            properties = tool.inputSchema["properties"]
            required = tool.inputSchema.get("required", [])

            tools_desc += "  Parameters:\n"
            for param_name, param_info in properties.items():
                param_type = param_info.get("type", "string")
                param_desc = param_info.get("description", "")
                is_required = param_name in required
                req_marker = " (required)" if is_required else ""

                tools_desc += (
                    f"    - {param_name} ({param_type}){req_marker}: {param_desc}\n"
                )

        tools_desc += "\n"

    return tools_desc


def build_system_prompt(user_system_prompt: str, tools: List[MCPTool]) -> str:
    """
    æ„å»ºåŒ…å«å·¥å…·ä¿¡æ¯çš„ç³»ç»Ÿæç¤ºè¯

    Args:
        user_system_prompt: ç”¨æˆ·å®šä¹‰çš„ç³»ç»Ÿæç¤ºè¯
        tools: MCPå·¥å…·åˆ—è¡¨

    Returns:
        å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯


    """
    if tools and len(tools) > 0:
        return (
            SYSTEM_PROMPT_TEMPLATE.replace(
                "{{ USER_SYSTEM_PROMPT }}", user_system_prompt
            )
            .replace("{{ TOOL_USE_EXAMPLES }}", TOOL_USE_EXAMPLES)
            .replace("{{ AVAILABLE_TOOLS }}", build_available_tools_prompt(tools))
        )

    return user_system_prompt


def parse_tool_use(
    content: str, mcp_tools: Optional[List[MCPTool]] = None
) -> List[ToolParseResult]:
    """
    è§£æLLMå“åº”ä¸­çš„å·¥å…·è°ƒç”¨


    Args:
        content: LLMå“åº”å†…å®¹
        mcp_tools: å¯ç”¨çš„MCPå·¥å…·åˆ—è¡¨

    Returns:
        è§£æå‡ºçš„å·¥å…·è°ƒç”¨åˆ—è¡¨
    """
    tools = []

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å·¥å…·è°ƒç”¨XML
    pattern = (
        r"<tool_use>\s*<tool_name>([^<]+)</tool_name>\s*"
        r"<parameters>([^<]*)</parameters>\s*</tool_use>"
    )
    matches = re.findall(pattern, content, re.DOTALL)

    for i, match in enumerate(matches):
        tool_name = match[0].strip()
        parameters_str = match[1].strip()

        try:
            # è§£æå‚æ•°
            parameters = json.loads(parameters_str.strip())

            # æŸ¥æ‰¾å¯¹åº”çš„MCPå·¥å…·ï¼ˆå¯é€‰éªŒè¯ï¼‰
            if mcp_tools:
                for tool in mcp_tools:
                    if tool.name == tool_name:
                        break

            # åˆ›å»ºå·¥å…·è°ƒç”¨
            tool_call = MCPToolCall(
                id=f"call_{i}", name=tool_name, arguments=parameters
            )

            # åˆ›å»ºè§£æç»“æœ
            parse_result = ToolParseResult(id=f"parse_{i}", tool=tool_call)

            tools.append(parse_result)

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse tool parameters for {tool_name}: {e}")
            continue

    return tools


async def call_mcp_tool(
    tool_call: MCPToolCall, mcp_tools: Optional[List[MCPTool]] = None
) -> MCPCallToolResponse:
    """
    è°ƒç”¨MCPå·¥å…·


    Args:
        tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯
        mcp_tools: å¯ç”¨çš„MCPå·¥å…·åˆ—è¡¨

    Returns:
        å·¥å…·è°ƒç”¨å“åº”
    """
    try:
        # æŸ¥æ‰¾å¯¹åº”çš„MCPå·¥å…·
        target_tool = None
        if mcp_tools:
            for tool in mcp_tools:
                if tool.name == tool_call.name:
                    target_tool = tool
                    break

        if not target_tool:
            logger.error(f"Tool not found: {tool_call.name}")
            return MCPCallToolResponse(
                content=[
                    {"type": "text", "text": f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å·¥å…· {tool_call.name}"}
                ],
                isError=True,
            )

        # è·å–æœåŠ¡å™¨é…ç½®
        server_config = get_server_config(target_tool.server_id)
        if not server_config:
            logger.error(f"Server config not found: {target_tool.server_id}")
            return MCPCallToolResponse(
                content=[
                    {
                        "type": "text",
                        "text": f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœåŠ¡å™¨é…ç½® {target_tool.server_id}",
                    }
                ],
                isError=True,
            )

        # è·å–MCPæœåŠ¡
        mcp_service = get_mcp_service()

        # è°ƒç”¨å·¥å…·
        result = await mcp_service.call_tool(
            server_config, tool_call.name, tool_call.arguments
        )

        # å¤„ç†è°ƒç”¨ç»“æœ
        if hasattr(result, "content"):
            # è½¬æ¢MCP SDKè¿”å›çš„å†…å®¹æ ¼å¼
            content = []
            for item in result.content:
                if hasattr(item, "text"):
                    # å¤„ç†TextContentå¯¹è±¡
                    content.append({"type": "text", "text": item.text})
                elif hasattr(item, "data"):
                    # å¤„ç†ImageContentå¯¹è±¡
                    content.append(
                        {
                            "type": "image",
                            "data": item.data,
                            "mimeType": getattr(item, "mimeType", "image/png"),
                        }
                    )
                elif isinstance(item, dict):
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼
                    content.append(item)
                else:
                    # å…¶ä»–æƒ…å†µï¼Œè½¬ä¸ºæ–‡æœ¬
                    content.append({"type": "text", "text": str(item)})
        else:
            content = [{"type": "text", "text": str(result)}]

        return MCPCallToolResponse(content=content, isError=False)

    except Exception as e:
        logger.error(f"Failed to call MCP tool {tool_call.name}: {e}")
        return MCPCallToolResponse(
            content=[{"type": "text", "text": f"å·¥å…·è°ƒç”¨å¤±è´¥ï¼š{str(e)}"}], isError=True
        )


def upsert_mcp_tool_response(
    tool_responses: List[MCPToolResponse],
    tool_response: MCPToolResponse,
    on_chunk: Optional[Callable] = None,
) -> None:
    """
    æ›´æ–°æˆ–æ’å…¥MCPå·¥å…·å“åº”


    Args:
        tool_responses: å·¥å…·å“åº”åˆ—è¡¨
        tool_response: æ–°çš„å·¥å…·å“åº”
        on_chunk: æµå¼å“åº”å›è°ƒå‡½æ•°
    """
    # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨ç›¸åŒIDçš„å“åº”
    for i, existing_response in enumerate(tool_responses):
        if existing_response.id == tool_response.id:
            tool_responses[i] = tool_response
            if on_chunk:
                on_chunk(
                    {
                        "text": (
                            f"[å·¥å…·æ›´æ–°] {tool_response.tool.name}: "
                            f"{tool_response.status}\n"
                        ),
                        "tool_response": tool_response.model_dump(),
                    }
                )
            return

    # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ æ–°çš„å“åº”
    tool_responses.append(tool_response)
    if on_chunk:
        on_chunk(
            {
                "text": f"[å·¥å…·è°ƒç”¨] {tool_response.tool.name}: {tool_response.status}\n",
                "tool_response": tool_response.model_dump(),
            }
        )


def default_convert_to_message(
    tool_call_id: str, response: MCPCallToolResponse, is_vision_model: bool = False
) -> ChatMessage:
    """
    é»˜è®¤çš„å·¥å…·è°ƒç”¨ç»“æœè½¬æ¢ä¸ºæ¶ˆæ¯çš„å‡½æ•°

    Args:
        tool_call_id: å·¥å…·è°ƒç”¨ID
        response: å·¥å…·è°ƒç”¨å“åº”
        is_vision_model: æ˜¯å¦ä¸ºè§†è§‰æ¨¡å‹

    Returns:
        è½¬æ¢åçš„èŠå¤©æ¶ˆæ¯
    """
    # æå–æ–‡æœ¬å†…å®¹
    text_content = ""
    for content_item in response.content:
        if content_item.get("type") == "text":
            text_content += content_item.get("text", "")
        elif content_item.get("type") == "image":
            text_content += f"[å›¾åƒ: {content_item.get('mimeType', 'image')}]"

    return ChatMessage(
        role="tool",
        content=text_content,
        tool_call_id=tool_call_id,
        metadata={"tool_response": response.model_dump(), "is_error": response.isError},
    )


async def parse_and_call_tools(
    content: str,
    tool_responses: List[MCPToolResponse],
    on_chunk: Optional[Callable],
    idx: int,
    convert_to_message: Optional[Callable] = None,
    mcp_tools: Optional[List[MCPTool]] = None,
    is_vision_model: bool = False,
) -> List[ChatMessage]:
    """
    è§£æLLMå“åº”ä¸­çš„å·¥å…·è°ƒç”¨å¹¶æ‰§è¡Œ

    Args:
        content: LLMå“åº”å†…å®¹
        tool_responses: å·¥å…·å“åº”åˆ—è¡¨
        on_chunk: æµå¼å“åº”å›è°ƒå‡½æ•°
        idx: æ¶ˆæ¯ç´¢å¼•
        convert_to_message: è½¬æ¢ä¸ºæ¶ˆæ¯çš„å‡½æ•°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤å‡½æ•°ï¼‰
        mcp_tools: å¯ç”¨çš„MCPå·¥å…·åˆ—è¡¨
        is_vision_model: æ˜¯å¦ä¸ºè§†è§‰æ¨¡å‹

    Returns:
        å·¥å…·è°ƒç”¨ç»“æœæ¶ˆæ¯åˆ—è¡¨
    """
    # ä½¿ç”¨é»˜è®¤è½¬æ¢å‡½æ•°å¦‚æœæœªæä¾›
    if convert_to_message is None:
        convert_to_message = default_convert_to_message

    tool_results: List[ChatMessage] = []

    # è§£æå·¥å…·ä½¿ç”¨
    tools = parse_tool_use(content, mcp_tools or [])
    if not tools or len(tools) == 0:
        return tool_results

    # ä¸ºæ¯ä¸ªå·¥å…·åˆ›å»ºåˆå§‹å“åº”
    for i, tool_parse_result in enumerate(tools):
        tool_response = MCPToolResponse(
            id=f"{tool_parse_result.id}-{idx}-{i}",
            tool=tool_parse_result.tool,
            status="invoking",
        )
        upsert_mcp_tool_response(tool_responses, tool_response, on_chunk)

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    images: List[str] = []

    async def execute_single_tool(
        tool_parse_result: ToolParseResult, i: int
    ) -> ChatMessage:
        """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨"""
        try:
            tool_call_response = await call_mcp_tool(tool_parse_result.tool, mcp_tools)

            # æ›´æ–°å·¥å…·å“åº”çŠ¶æ€
            tool_response = MCPToolResponse(
                id=f"{tool_parse_result.id}-{idx}-{i}",
                tool=tool_parse_result.tool,
                status="done" if not tool_call_response.isError else "error",
                content=tool_call_response.content,
                error=None if not tool_call_response.isError else "å·¥å…·è°ƒç”¨å¤±è´¥",
            )
            upsert_mcp_tool_response(tool_responses, tool_response, on_chunk)

            # å¤„ç†å›¾åƒå†…å®¹
            for content_item in tool_call_response.content:
                if content_item.get("type") == "image" and content_item.get("data"):
                    mime_type = content_item.get("mimeType", "image/png")
                    images.append(f"data:{mime_type};base64,{content_item['data']}")

            # å‘é€å›¾åƒæ›´æ–°
            if images and on_chunk:
                on_chunk(
                    {
                        "text": "\n",
                        "generateImage": {"type": "base64", "images": images},
                    }
                )

            # è½¬æ¢ä¸ºèŠå¤©æ¶ˆæ¯
            return convert_to_message(
                tool_parse_result.tool.id, tool_call_response, is_vision_model
            )

        except Exception as e:
            logger.error(f"Tool execution failed: {e}")
            # åˆ›å»ºé”™è¯¯å“åº”
            error_response = MCPCallToolResponse(
                content=[{"type": "text", "text": f"å·¥å…·æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"}],
                isError=True,
            )
            return convert_to_message(
                tool_parse_result.tool.id, error_response, is_vision_model
            )

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·
    tool_promises = [execute_single_tool(tool, i) for i, tool in enumerate(tools)]

    tool_results.extend(await asyncio.gather(*tool_promises))
    return tool_results


class AIProvider:
    """
    AI Providerï¼šè´Ÿè´£LLMè°ƒç”¨å’Œå“åº”å¤„ç†
    """

    def __init__(self):
        # è®¾ç½®LiteLLMçš„åŸºç¡€URLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if os.getenv("OPENAI_API_BASE"):
            os.environ["OPENAI_API_BASE"] = os.getenv("OPENAI_API_BASE")
            logger.info(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰APIåŸºç¡€URL: {os.getenv('OPENAI_API_BASE')}")

        # è·å–é»˜è®¤æ¨¡å‹é…ç½®
        self.default_model = os.getenv(
            "MODEL_NAME", os.getenv("DEFAULT_MODEL", "gpt-3.5-turbo")
        )
        self.default_temperature = float(os.getenv("DEFAULT_TEMPERATURE", "0.7"))

        logger.info(f"âœ… AI Provideråˆå§‹åŒ–å®Œæˆï¼Œé»˜è®¤æ¨¡å‹: {self.default_model}")

    async def completions(
        self,
        messages: List[ChatMessage],
        model: Optional[str] = None,
        mcp_tools: Optional[List[MCPTool]] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        on_chunk: Optional[Callable] = None,
    ) -> ChatResponse:
        """
        æ‰§è¡ŒLLM completionsè°ƒç”¨ï¼Œé›†æˆMCPå·¥å…·

        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é»˜è®¤å€¼ï¼‰
            mcp_tools: MCPå·¥å…·åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é»˜è®¤å€¼ï¼‰
            stream: æ˜¯å¦æµå¼å“åº”
            on_chunk: æµå¼å“åº”å›è°ƒå‡½æ•°

        Returns:
            èŠå¤©å“åº”
        """
        # ä½¿ç”¨æä¾›çš„å‚æ•°æˆ–ç¯å¢ƒå˜é‡é»˜è®¤å€¼
        actual_model = model or self.default_model
        actual_temperature = (
            temperature if temperature is not None else self.default_temperature
        )

        logger.info(f"[AI] Starting completions with model: {actual_model}")
        logger.info(f"[AI] MCP tools available: {len(mcp_tools) if mcp_tools else 0}")

        # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
        processed_messages = []

        for msg in messages:
            # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ MCPå·¥å…·ä¿¡æ¯
            if msg.role == "system":
                if mcp_tools and len(mcp_tools) > 0:
                    enhanced_content = build_system_prompt(msg.content, mcp_tools)
                    processed_messages.append(
                        {"role": "system", "content": enhanced_content}
                    )
                    logger.info(
                        f"[AI] Enhanced system message with {len(mcp_tools)} MCP tools"
                    )
                else:
                    processed_messages.append(
                        {"role": msg.role, "content": msg.content}
                    )
            else:
                processed_messages.append({"role": msg.role, "content": msg.content})

        try:
            # è°ƒç”¨LiteLLM
            if stream:
                return await self._stream_completion(
                    processed_messages,
                    actual_model,
                    actual_temperature,
                    mcp_tools,
                    on_chunk,
                )
            else:
                return await self._sync_completion(
                    processed_messages, actual_model, actual_temperature, mcp_tools
                )

        except Exception as e:
            logger.error(f"[AI] LLM completion failed: {e}")
            error_message = ChatMessage(
                role="assistant",
                content=f"æŠ±æ­‰ï¼ŒAIè°ƒç”¨å¤±è´¥ï¼š{str(e)}",
                metadata={"error": str(e)},
            )
            return ChatResponse(
                message=error_message,
                usage={"error": True},
                metrics={"step": 3, "status": "ai_completion_error"},
            )

    async def _sync_completion(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        mcp_tools: Optional[List[MCPTool]],
    ) -> ChatResponse:
        """åŒæ­¥å®ŒæˆLLMè°ƒç”¨"""
        response = await litellm.acompletion(
            model=model, messages=messages, temperature=temperature
        )

        # æå–å“åº”å†…å®¹
        content = response.choices[0].message.content
        usage = response.usage.dict() if response.usage else {}

        # è§£æå·¥å…·è°ƒç”¨
        tool_calls = parse_tool_use(content, mcp_tools)

        response_message = ChatMessage(
            role="assistant",
            content=content,
            metadata={
                "mcp_tools": (
                    [tool.tool.dict() for tool in tool_calls] if mcp_tools else []
                ),
                "tool_calls_detected": len(tool_calls),
                "parsed_tool_calls": tool_calls,
            },
            tool_calls=[tool.tool for tool in tool_calls] if tool_calls else None,
        )

        return ChatResponse(
            message=response_message,
            usage=usage,
            metrics={
                "step": 3,
                "status": "ai_completion_success",
                "tool_calls_detected": len(tool_calls),
            },
        )

    async def _stream_completion(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        mcp_tools: Optional[List[MCPTool]],
        on_chunk: Optional[Callable],
    ) -> ChatResponse:
        """æµå¼å®ŒæˆLLMè°ƒç”¨"""
        # æµå¼å®ç°çš„å ä½ç¬¦
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šå®ç°çœŸæ­£çš„æµå¼å¤„ç†
        logger.info(
            "[AI] Stream completion not fully implemented, falling back to sync"
        )
        return await self._sync_completion(messages, model, temperature, mcp_tools)


def getMcpServerByTool(tool: MCPTool) -> Optional[MCPServer]:
    """
    æ ¹æ®å·¥å…·è·å–å¯¹åº”çš„MCPæœåŠ¡å™¨é…ç½®


    Args:
        tool: MCPå·¥å…·å¯¹è±¡

    Returns:
        å¯¹åº”çš„æœåŠ¡å™¨é…ç½®ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    return get_server_config(tool.server_id)


async def callMCPTool(
    tool_name: str, arguments: Dict[str, Any], mcp_tools: List[MCPTool]
) -> MCPCallToolResponse:
    """
    ä½¿ç”¨å·¥å…·åç§°å’Œå‚æ•°è°ƒç”¨MCPå·¥å…·
    è¿™æ˜¯Task7çš„æ ¸å¿ƒåŠŸèƒ½ï¼šæ¥æ”¶LLMç”Ÿæˆçš„å·¥å…·è°ƒç”¨å‚æ•°ï¼Œå®é™…æ‰§è¡ŒMCPå·¥å…·

    Args:
        tool_name: å·¥å…·åç§°
        arguments: å·¥å…·è°ƒç”¨å‚æ•°ï¼ˆä»LLMå“åº”ä¸­è§£æå‡ºæ¥çš„ï¼‰
        mcp_tools: å¯ç”¨çš„MCPå·¥å…·åˆ—è¡¨

    Returns:
        å·¥å…·è°ƒç”¨å“åº”
    """
    logger.info(f"[MCP] Calling Tool: {tool_name} with args: {arguments}")

    try:
        # æ¸…ç†å‚æ•°ï¼šå»é™¤ç©ºå­—ç¬¦ä¸²ã€Noneå€¼å’Œç©ºåˆ—è¡¨
        cleaned_arguments = {}
        for key, value in arguments.items():
            # è·³è¿‡ç©ºå€¼å‚æ•°
            if value is None:
                continue
            if isinstance(value, str) and value.strip() == "":
                continue
            if isinstance(value, list) and len(value) == 0:
                continue
            # ä¿ç•™æœ‰æ„ä¹‰çš„å‚æ•°
            cleaned_arguments[key] = value

        logger.info(f"[MCP] Cleaned args: {cleaned_arguments}")

        # æŸ¥æ‰¾å¯¹åº”çš„MCPå·¥å…·å®šä¹‰
        target_tool = None
        for tool in mcp_tools:
            if tool.name == tool_name:
                target_tool = tool
                break

        if not target_tool:
            raise Exception(f"Tool not found: {tool_name}")

        # è·å–æœåŠ¡å™¨é…ç½®
        server = getMcpServerByTool(target_tool)
        if not server:
            raise Exception(f"Server not found for tool: {tool_name}")

        # è·å–MCPæœåŠ¡
        mcp_service = get_mcp_service()

        # ä½¿ç”¨æ¸…ç†åçš„å‚æ•°è°ƒç”¨å·¥å…·
        result = await mcp_service.call_tool(server, tool_name, cleaned_arguments)

        # å¤„ç†è°ƒç”¨ç»“æœ
        if hasattr(result, "content"):
            # è½¬æ¢MCP SDKè¿”å›çš„å†…å®¹æ ¼å¼
            content = []
            for item in result.content:
                if hasattr(item, "text"):
                    # å¤„ç†TextContentå¯¹è±¡
                    content.append({"type": "text", "text": item.text})
                elif hasattr(item, "data"):
                    # å¤„ç†ImageContentå¯¹è±¡
                    content.append(
                        {
                            "type": "image",
                            "data": item.data,
                            "mimeType": getattr(item, "mimeType", "image/png"),
                        }
                    )
                elif isinstance(item, dict):
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼
                    content.append(item)
                else:
                    # å…¶ä»–æƒ…å†µï¼Œè½¬ä¸ºæ–‡æœ¬
                    content.append({"type": "text", "text": str(item)})
        else:
            content = [{"type": "text", "text": str(result)}]

        logger.info(f"[MCP] Tool called successfully: {tool_name}")
        return MCPCallToolResponse(content=content, isError=False)

    except Exception as e:
        logger.error(f"[MCP] Error calling Tool: {tool_name}: {e}")
        return MCPCallToolResponse(
            content=[
                {"type": "text", "text": (f"Error calling tool {tool_name}: {str(e)}")}
            ],
            isError=True,
        )


async def execute_mcp_tool_calls(
    tool_calls: List[ToolParseResult],
    mcp_tools: List[MCPTool],
    on_progress: Optional[Callable] = None,
) -> List[MCPCallToolResponse]:
    """
    æ‰¹é‡æ‰§è¡ŒMCPå·¥å…·è°ƒç”¨
    è¿™æ˜¯å®Œæ•´çš„Task7æµç¨‹ï¼šæ¥æ”¶è§£æçš„å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼Œæ‰§è¡Œå¹¶è¿”å›ç»“æœ

    Args:
        tool_calls: ä»LLMå“åº”ä¸­è§£æå‡ºçš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        mcp_tools: å¯ç”¨çš„MCPå·¥å…·åˆ—è¡¨
        on_progress: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        å·¥å…·è°ƒç”¨å“åº”åˆ—è¡¨
    """
    results = []

    for i, tool_call in enumerate(tool_calls):
        if on_progress:
            on_progress(f"æ‰§è¡Œå·¥å…· {i+1}/{len(tool_calls)}: {tool_call.tool.name}")

        # è°ƒç”¨å•ä¸ªå·¥å…·
        response = await callMCPTool(
            tool_call.tool.name, tool_call.tool.arguments, mcp_tools
        )
        results.append(response)

        if on_progress:
            status = "æˆåŠŸ" if not response.isError else "å¤±è´¥"
            on_progress(f"å·¥å…· {tool_call.tool.name} æ‰§è¡Œ{status}")

    return results


async def complete_mcp_workflow(
    messages: List[ChatMessage],
    enabled_servers: List[MCPServer],
    model: Optional[str] = None,
    max_iterations: int = 3,
    on_progress: Optional[Callable] = None,
) -> ChatResponse:
    """
    å®Œæ•´çš„MCPå·¥ä½œæµç¨‹ï¼š
    1. ä»å¯ç”¨çš„æœåŠ¡å™¨è·å–å·¥å…·åˆ—è¡¨
    2. LLMç”Ÿæˆå·¥å…·è°ƒç”¨
    3. æ‰§è¡Œå·¥å…·
    4. è¿”å›ç»“æœç»™LLM
    5. ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

    Args:
        messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
        enabled_servers: å¯ç”¨çš„MCPæœåŠ¡å™¨åˆ—è¡¨
        model: LLMæ¨¡å‹åç§°
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        on_progress: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        æœ€ç»ˆçš„èŠå¤©å“åº”
    """
    if on_progress:
        on_progress("ğŸš€ å¼€å§‹MCPå®Œæ•´å·¥ä½œæµç¨‹")

    # 1. ä»å¯ç”¨çš„æœåŠ¡å™¨è·å–å·¥å…·åˆ—è¡¨
    if on_progress:
        on_progress(f"ğŸ“¡ ä» {len(enabled_servers)} ä¸ªæœåŠ¡å™¨è·å–å·¥å…·åˆ—è¡¨")

    from .mcp_chat_handler import MCPToolCollector

    tool_collector = MCPToolCollector()
    mcp_tools = await tool_collector.collect_mcp_tools(enabled_servers)

    if on_progress:
        on_progress(f"âœ… è·å–åˆ° {len(mcp_tools)} ä¸ªå¯ç”¨å·¥å…·")

    # 2. æ‰§è¡Œå®Œæ•´çš„å¯¹è¯æµç¨‹
    provider = AIProvider()
    current_messages = messages.copy()

    # æ‰“å°åˆå§‹æ¶ˆæ¯
    if on_progress:
        on_progress("=" * 50)
        on_progress("ğŸ“‹ åˆå§‹å¯¹è¯æ¶ˆæ¯:")
        for i, msg in enumerate(current_messages):
            on_progress(f"  [{i+1}] Role: {msg.role}")
            on_progress(
                f"      Content: {msg.content[:200]}{'...' if len(msg.content) > 200 else ''}"
            )
        on_progress("=" * 50)

    for iteration in range(max_iterations):
        if on_progress:
            on_progress(f"ğŸ”„ å¼€å§‹ç¬¬ {iteration + 1} è½®å¯¹è¯")
            on_progress(f"ğŸ“¨ å½“å‰æ¶ˆæ¯å†å²é•¿åº¦: {len(current_messages)}")

        # è°ƒç”¨LLMç”Ÿæˆå“åº”
        llm_response = await provider.completions(
            current_messages, model=model, mcp_tools=mcp_tools
        )

        # æ‰“å°LLMå“åº”
        if on_progress:
            on_progress("-" * 30)
            on_progress(f"ğŸ¤– LLMç¬¬{iteration + 1}è½®å“åº”:")
            on_progress(f"   ğŸ“ Content: {llm_response.message.content}")
            on_progress(f"   ğŸ“Š Usage: {llm_response.usage}")
            on_progress("-" * 30)

        # è§£æå·¥å…·è°ƒç”¨
        tool_calls = parse_tool_use(llm_response.message.content, mcp_tools)

        if not tool_calls:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»“æœ
            if on_progress:
                on_progress("âœ… LLMæœªè¯·æ±‚å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç­”æ¡ˆ")
                on_progress("=" * 50)
            return llm_response

        if on_progress:
            on_progress(f"ğŸ”§ è§£æåˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
            for i, tool_call in enumerate(tool_calls):
                on_progress(f"   [{i+1}] å·¥å…·: {tool_call.tool.name}")
                on_progress(f"       å‚æ•°: {tool_call.tool.arguments}")

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_results = await execute_mcp_tool_calls(tool_calls, mcp_tools, on_progress)

        # å°†LLMå“åº”æ·»åŠ åˆ°æ¶ˆæ¯å†å²
        current_messages.append(llm_response.message)

        # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        for i, (tool_call, result) in enumerate(zip(tool_calls, tool_results)):
            # æå–å·¥å…·ç»“æœçš„æ–‡æœ¬å†…å®¹
            result_text = ""
            if result.content:
                for content_item in result.content:
                    if content_item.get("type") == "text":
                        result_text += content_item.get("text", "")
                    else:
                        result_text += str(content_item)

            # åˆ›å»ºæ›´æ¸…æ™°çš„å·¥å…·ç»“æœæ¶ˆæ¯ï¼Œè®©LLMæ˜ç¡®çŸ¥é“å·¥å…·å·²æ‰§è¡Œå®Œæˆ
            tool_message = ChatMessage(
                role="user",  # æ”¹ä¸ºuserè§’è‰²ï¼Œè®©LLMæ›´å®¹æ˜“ç†è§£è¿™æ˜¯è¾“å…¥ä¿¡æ¯
                content=(
                    f"å·¥å…·è°ƒç”¨ç»“æœï¼š\nå·¥å…·åç§°ï¼š{tool_call.tool.name}\n"
                    f"æ‰§è¡ŒçŠ¶æ€ï¼š{'æˆåŠŸ' if not result.isError else 'å¤±è´¥'}\n"
                    f"ç»“æœå†…å®¹ï¼š\n{result_text}"
                ),
                metadata={
                    "tool_name": tool_call.tool.name,
                    "tool_result": result.model_dump(),
                    "is_error": result.isError,
                    "message_type": "tool_result",
                },
            )
            current_messages.append(tool_message)

            # æ‰“å°å·¥å…·ç»“æœæ¶ˆæ¯
            if on_progress:
                on_progress("ğŸ“¤ æ·»åŠ åˆ°æ¶ˆæ¯å†å² - å·¥å…·ç»“æœ:")
                on_progress(f"   ğŸ”§ å·¥å…·: {tool_call.tool.name}")
                on_progress(
                    f"   ğŸ“„ ç»“æœ: {result_text[:150]}"
                    f"{'...' if len(result_text) > 150 else ''}"
                )

        if on_progress:
            on_progress(f"ğŸ”„ ç¬¬ {iteration + 1} è½®å·¥å…·è°ƒç”¨å®Œæˆï¼Œç»§ç»­å¯¹è¯")
            on_progress(f"ğŸ“¨ æ›´æ–°åæ¶ˆæ¯å†å²é•¿åº¦: {len(current_messages)}")

    # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›æœ€åä¸€æ¬¡å“åº”
    if on_progress:
        on_progress(f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œè¿”å›å½“å‰ç»“æœ")
        on_progress("=" * 50)

    return llm_response
